{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Train NF-ResNet on Cifar-10 using PyTorch Lightning",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/quanganh1999/NF-Net-on-CIFAR/blob/main/Train_NF_ResNet_on_Cifar_100_using_PyTorch_Lightning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MirDQLgiwyi2"
      },
      "source": [
        "## ‚öôÔ∏è Imports and Setups"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bb5l144ScmdM"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQUxq6oRtUP9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Qo4vWV4NbiO"
      },
      "source": [
        "%cd drive/MyDrive/NF-Resnet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evscgXTFwszZ"
      },
      "source": [
        "%%capture\n",
        "# Install pytorch lighting\n",
        "!pip install pytorch-lightning --quiet\n",
        "# Install weights and biases\n",
        "!pip install wandb --quiet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ugRAnKKYtzU"
      },
      "source": [
        "!pip install lightning-bolts[\"extra\"] --quiet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wX7QXaoVz3OC"
      },
      "source": [
        "!git clone https://github.com/rwightman/pytorch-image-models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSsjHKVnd2Dg"
      },
      "source": [
        "!pip install git+https://github.com/rwightman/pytorch-image-models.git --quiet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieJ6Kl7ixBFT"
      },
      "source": [
        "# regular imports\n",
        "import sys\n",
        "sys.path.append(\"pytorch-image-models\")\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "# pytorch related imports \n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.datasets.utils import download_url\n",
        "\n",
        "# import for nfnet\n",
        "import timm\n",
        "\n",
        "# lightning related imports\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.metrics.functional import accuracy\n",
        "from pytorch_lightning.callbacks import Callback\n",
        "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "\n",
        "# sklearn related imports\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# import wandb and login\n",
        "import wandb\n",
        "wandb.login()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IcAjtkapCav"
      },
      "source": [
        "#@title\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision.datasets import CIFAR100\n",
        "from pytorch_lightning.utilities.seed import seed_everything\n",
        "import pl_bolts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKp7sN7IDHcz"
      },
      "source": [
        "seed_everything(1999)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_11_ucfHiXN"
      },
      "source": [
        "## üé® Using DataModules - `Clatech101DataModule`\n",
        "\n",
        "DataModules are a way of decoupling data-related hooks from the `LightningModule` so you can develop dataset agnostic models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoQ_VqcLwtSl"
      },
      "source": [
        "class CIFAR100Data(pl.LightningDataModule):\n",
        "      def __init__(self, batch_size, data_dir: str = './'):\n",
        "        super().__init__()\n",
        "        self.data_dir = data_dir\n",
        "        self.batch_size = batch_size        \n",
        "        self.mean = (0.4914, 0.4822, 0.4465)\n",
        "        self.std = (0.2471, 0.2435, 0.2616)\n",
        "\n",
        "        #augmentation (use other strong augmentation)\n",
        "        # self.train_transform = transforms.Compose(\n",
        "        #     [\n",
        "        #         transforms.RandomCrop(32, padding=4),\n",
        "        #         transforms.RandomHorizontalFlip(),\n",
        "        #         transforms.ToTensor(),\n",
        "        #         transforms.Normalize(self.mean, self.std),\n",
        "        #     ]\n",
        "        # )\n",
        "        self.train_transform = transforms.Compose([\n",
        "          transforms.RandomResizedCrop((256, 256), scale=(0.05, 1.0)),\n",
        "          transforms.ToTensor(),\n",
        "          transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "        ])\n",
        "   \n",
        "        #need to resize for more acc ??\n",
        "        # self.test_transform = transforms.Compose(\n",
        "        #     [\n",
        "        #         transforms.ToTensor(),\n",
        "        #         transforms.Normalize(self.mean, self.std),\n",
        "        #     ]\n",
        "        # )\n",
        "        self.test_transform = transforms.Compose([\n",
        "        transforms.Resize((320, 320)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "        ])\n",
        "        # self.dims = (3, 32, 32)\n",
        "        self.num_classes = 100\n",
        "\n",
        "      def prepare_data(self):\n",
        "        # download \n",
        "        CIFAR100(self.data_dir, train=True, download=True)\n",
        "        CIFAR100(self.data_dir, train=False, download=True)    \n",
        "\n",
        "      def setup(self, stage=None):        \n",
        "        if stage == 'fit' or stage is None:\n",
        "            # load the dataset    \n",
        "            self.cifar_full_train = CIFAR100(self.data_dir, train=True, transform=self.train_transform)\n",
        "            self.cifar_full_val = CIFAR100(self.data_dir, train=True, transform=self.test_transform)             \n",
        "            num_train = len(self.cifar_full_train)\n",
        "            indices = list(range(num_train))\n",
        "            split = int(np.floor(0.1 * num_train))\n",
        "            np.random.seed(1999)\n",
        "            np.random.shuffle(indices)\n",
        "\n",
        "            #train\n",
        "            train_idx, valid_idx = indices[split:], indices[:split]\n",
        "            self.train_sampler = SubsetRandomSampler(train_idx)\n",
        "            self.valid_sampler = SubsetRandomSampler(valid_idx)            \n",
        "\n",
        "        # Assign test dataset for use in dataloader(s)\n",
        "        if stage == 'test' or stage is None:\n",
        "            self.cifar_test = CIFAR100(self.data_dir, train=False, transform=self.test_transform)        \n",
        "\n",
        "      def train_dataloader(self):\n",
        "        #return DataLoader(self.cifar_full_train, batch_size=self.batch_size, sampler=self.train_sampler)\n",
        "        return DataLoader(self.cifar_full_train, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "      def val_dataloader(self):      \n",
        "        #return DataLoader(self.cifar_full_val, batch_size=self.batch_size, sampler=self.valid_sampler)\n",
        "        return DataLoader(self.cifar_test, batch_size=self.batch_size)\n",
        "\n",
        "      def test_dataloader(self):\n",
        "        return DataLoader(self.cifar_test, batch_size=self.batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkeBFJBkYJr1"
      },
      "source": [
        "## üì≤ Callbacks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBz4YZvYg-jc"
      },
      "source": [
        "#### üöè Earlystopping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9gvY3pfYLOm"
      },
      "source": [
        "early_stop_callback = EarlyStopping(\n",
        "   monitor='val_loss',\n",
        "   patience=3,\n",
        "   verbose=False,\n",
        "   mode='min'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dhQJeOXhAmC"
      },
      "source": [
        "#### üõÉ Custom Callback - `ImagePredictionLogger`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GdiZ0b2FTfF"
      },
      "source": [
        "class ImagePredictionLogger(Callback):\n",
        "    def __init__(self, val_samples, num_samples=32):\n",
        "        super().__init__()\n",
        "        self.num_samples = num_samples\n",
        "        self.val_imgs, self.val_labels = val_samples\n",
        "        \n",
        "    def on_validation_epoch_end(self, trainer, pl_module):\n",
        "        val_imgs = self.val_imgs.to(device=pl_module.device)\n",
        "        val_labels = self.val_labels.to(device=pl_module.device)\n",
        "       \n",
        "        logits = pl_module(val_imgs)\n",
        "        preds = torch.argmax(logits, -1)\n",
        "        \n",
        "        trainer.logger.experiment.log({\n",
        "            \"examples\":[wandb.Image(x, caption=f\"Pred:{pred}, Label:{y}\") \n",
        "                           for x, pred, y in zip(val_imgs[:self.num_samples], \n",
        "                                                 preds[:self.num_samples], \n",
        "                                                 val_labels[:self.num_samples])]\n",
        "            })"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpqnhW1xFAGH"
      },
      "source": [
        "#### üíæ Model Checkpoint Callback"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUiYNQbEG01y"
      },
      "source": [
        "MODEL_CKPT_PATH = './model'\n",
        "# MODEL_CKPT = 'model-{epoch:02d}-{val_loss:.2f}'\n",
        "MODEL_CKPT = 'model-{epoch:02d}-{val_acc:.3f}'\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    monitor='val_acc',\n",
        "    dirpath = MODEL_CKPT_PATH,\n",
        "    filename=MODEL_CKPT,\n",
        "    save_top_k=3,\n",
        "    save_last = True,\n",
        "    mode='max')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AegEWlwjS7Gv"
      },
      "source": [
        "## üé∫ Define The Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKBN_rFfP0mL"
      },
      "source": [
        "class LitModel(pl.LightningModule):\n",
        "    def __init__(self, input_shape, num_classes, learning_rate=2e-4):\n",
        "        super().__init__()\n",
        "        \n",
        "        # log hyperparameters\n",
        "        self.save_hyperparameters()\n",
        "        self.learning_rate = learning_rate\n",
        "        self.dim = input_shape\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        #train from scratch\n",
        "        # self.classifier = timm.create_model('nf_resnet50', pretrained=False, num_classes = num_classes)\n",
        "\n",
        "        #transfer learning\n",
        "        #fine-tuning\n",
        "        self.model = timm.create_model(\"eca_nfnet_l1\", pretrained=True, num_classes=num_classes) #256 input 320 test\n",
        "\n",
        "        #feature exactor (freezee all layers except the last layer)\n",
        "        # self.model = timm.create_model(\"eca_nfnet_l1\", pretrained=True)        \n",
        "        # for param in self.model.parameters():\n",
        "        #     param.requires_grad = False\n",
        "        # self.model.reset_classifier(num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = F.log_softmax(self.model.forward(x), dim=1)\n",
        "      return x\n",
        "\n",
        "\n",
        "    # logic for a single training step\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = F.cross_entropy(logits, y)\n",
        "        \n",
        "        # training metrics\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        acc = accuracy(preds, y)\n",
        "        self.log('train_loss', loss, on_step=True, on_epoch=True, logger=True)\n",
        "        self.log('train_acc', acc, on_step=True, on_epoch=True, logger=True)\n",
        "        \n",
        "        return loss\n",
        "\n",
        "    # logic for a single validation step\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = F.cross_entropy(logits, y)\n",
        "\n",
        "        # validation metrics\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        acc = accuracy(preds, y)\n",
        "        self.log('val_loss', loss, prog_bar=True)\n",
        "        self.log('val_acc', acc, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    # logic for a single testing step\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = F.cross_entropy(logits, y)\n",
        "        \n",
        "        # validation metrics\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        acc = accuracy(preds, y)\n",
        "        self.log('test_loss', loss, prog_bar=True)\n",
        "        self.log('test_acc', acc, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
        "        optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-6, nesterov = True)\n",
        "        scheduler = pl_bolts.optimizers.lr_scheduler.LinearWarmupCosineAnnealingLR(optimizer, warmup_epochs = 5, max_epochs = 60)\n",
        "        # return optimizer\n",
        "        return [optimizer], [scheduler]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceYFNcOuh9fP"
      },
      "source": [
        "## ‚ö° Train and Evaluate The Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvY-POwDT3Fi"
      },
      "source": [
        "# Init our data pipeline\n",
        "BATCH_SIZE = 64\n",
        "# dm = CIFAR10Data(batch_size=BATCH_SIZE)\n",
        "dm = CIFAR100Data(batch_size=BATCH_SIZE)\n",
        "# To access the x_dataloader we need to call prepare_data and setup.\n",
        "dm.prepare_data()\n",
        "dm.setup()\n",
        "\n",
        "# Samples required by the custom ImagePredictionLogger callback to log image predictions.\n",
        "val_samples = next(iter(dm.val_dataloader()))\n",
        "val_imgs, val_labels = val_samples[0], val_samples[1]\n",
        "val_imgs.shape, val_labels.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjZi8Ya-fAa9"
      },
      "source": [
        "# Init our model\n",
        "model = LitModel((3, 256, 256), 100)\n",
        "\n",
        "# Initialize wandb logger\n",
        "wandb_logger = WandbLogger(project='nfnet', job_type='train-nf-resnet')\n",
        "\n",
        "# Initialize a trainer\n",
        "trainer = pl.Trainer(max_epochs=60,\n",
        "                     progress_bar_refresh_rate=5, \n",
        "                     gpus=1, \n",
        "                     logger=wandb_logger,\n",
        "                     #early_stop_callback,\n",
        "                     callbacks=[ImagePredictionLogger(val_samples), checkpoint_callback]\n",
        "                     )\n",
        "\n",
        "# Train the model ‚ö°üöÖ‚ö°\n",
        "trainer.fit(model, dm) \n",
        "\n",
        "# Evaluate the model on the held out test set ‚ö°‚ö°\n",
        "trainer.test()\n",
        "\n",
        "# Close wandb run\n",
        "wandb.finish()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaPgH28gLskz"
      },
      "source": [
        "#resume from checkpoint:\n",
        "model = LitModel((3, 256, 256), 10)\n",
        "\n",
        "# Initialize wandb logger\n",
        "wandb_logger = WandbLogger(project='nfnet', job_type='train-nf-resnet')\n",
        "\n",
        "# Initialize a trainer\n",
        "trainer = pl.Trainer(max_epochs=60,\n",
        "                     progress_bar_refresh_rate=5, \n",
        "                     gpus=1, \n",
        "                     logger=wandb_logger,\n",
        "                     #early_stop_callback,\n",
        "                     callbacks=[ImagePredictionLogger(val_samples), checkpoint_callback],\n",
        "                     resume_from_checkpoint='model/last.ckpt'\n",
        "                     )\n",
        "\n",
        "# Train the model ‚ö°üöÖ‚ö°\n",
        "trainer.fit(model, dm) \n",
        "\n",
        "# Evaluate the model on the held out test set ‚ö°‚ö°\n",
        "trainer.test()\n",
        "\n",
        "# Close wandb run\n",
        "wandb.finish()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tp8V5XjKoQEc"
      },
      "source": [
        "#Test with checkpoint\n",
        "LitModel((3, 32, 32), 10)\n",
        "model = LitModel.load_from_checkpoint(checkpoint_path='./model/model2-epoch=41-val_acc=0.974.ckpt')\n",
        "\n",
        "# init trainer with whatever options\n",
        "trainer = trainer = pl.Trainer(gpus=1)\n",
        "\n",
        "# test (pass in the model)\n",
        "trainer.test(model, datamodule = dm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1FjXlc9ogTh"
      },
      "source": [
        "%ls model/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEhB5NogaJho"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}